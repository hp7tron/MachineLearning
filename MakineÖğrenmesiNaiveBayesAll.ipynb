{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_smileys():\n",
    "  return {\n",
    "        \":‑)\":\"gülücük\",\n",
    "        \":-]\":\"gülücük\",\n",
    "        \":-3\":\"gülücük\",\n",
    "        \":->\":\"gülücük\",\n",
    "        \"8-)\":\"gülücük\",\n",
    "        \":-}\":\"gülücük\",\n",
    "        \":)\":\"gülücük\",\n",
    "        \":]\":\"gülücük\",\n",
    "        \":3\":\"gülücük\",\n",
    "        \":>\":\"gülücük\",\n",
    "        \"8)\":\"gülücük\",\n",
    "        \":}\":\"gülücük\",\n",
    "        \":o)\":\"gülücük\",\n",
    "        \":c)\":\"gülücük\",\n",
    "        \":^)\":\"gülücük\",\n",
    "        \"=]\":\"gülücük\",\n",
    "        \"=)\":\"gülücük\",\n",
    "        \":-))\":\"gülücük\",\n",
    "        \":‑D\":\"gülücük\",\n",
    "        \"8‑D\":\"gülücük\",\n",
    "        \"x‑D\":\"gülücük\",\n",
    "        \"X‑D\":\"gülücük\",\n",
    "        \":D\":\"gülücük\",\n",
    "        \"8D\":\"gülücük\",\n",
    "        \"xD\":\"gülücük\",\n",
    "        \"XD\":\"gülücük\",\n",
    "        \":‑(\":\"üzgün\",\n",
    "        \":‑c\":\"üzgün\",\n",
    "        \":‑<\":\"üzgün\",\n",
    "        \":‑[\":\"üzgün\",\n",
    "        \":(\":\"üzgün\",\n",
    "        \":c\":\"üzgün\",\n",
    "        \":<\":\"üzgün\",\n",
    "        \":[\":\"üzgün\",\n",
    "        \":-||\":\"üzgün\",\n",
    "        \">:[\":\"üzgün\",\n",
    "        \":{\":\"üzgün\",\n",
    "        \":@\":\"üzgün\",\n",
    "        \">:(\":\"üzgün\",\n",
    "        \":'‑(\":\"üzgün\",\n",
    "        \":'(\":\"üzgün\",\n",
    "        \":‑P\":\"eğlenceli\",\n",
    "        \"X‑P\":\"eğlenceli\",\n",
    "        \"x‑p\":\"eğlenceli\",\n",
    "        \":‑p\":\"eğlenceli\",\n",
    "        \":‑Þ\":\"eğlenceli\",\n",
    "        \":‑þ\":\"eğlenceli\",\n",
    "        \":‑b\":\"eğlenceli\",\n",
    "        \":P\":\"eğlenceli\",\n",
    "        \"XP\":\"eğlenceli\",\n",
    "        \"xp\":\"eğlenceli\",\n",
    "        \":p\":\"eğlenceli\",\n",
    "        \":Þ\":\"eğlenceli\",\n",
    "        \":þ\":\"eğlenceli\",\n",
    "        \":b\":\"eğlenceli\",\n",
    "        \"<3\":\"sevgi\"\n",
    "        }\n",
    "def cumleyiarindir(tweet):\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "    #Special case not handled previously.\n",
    "    tweet = tweet.replace('\\x92',\"'\")\n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    #Strip accents\n",
    "   \n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . Klasör tweet sayısı : 756\n",
      "2 . Klasör tweet sayısı : 1287\n",
      "3 . Klasör tweet sayısı : 957\n"
     ]
    }
   ],
   "source": [
    "sentences=[]\n",
    "labels=[]\n",
    "\n",
    "#folders=[ '1','2', '3','4','5', '6','7','8', '9','10','11', '12','13','14', '15','16','17', '18','19','20']\n",
    "folders=[ '1','2', '3'] \n",
    "filename='/Users/hp7tron/MakineOgrenmesiFinal/raw_texts'\n",
    "for x in folders:\n",
    "    tweet_count=0 \n",
    "    path=os.path.join(filename, x)        \n",
    "    for t in os.listdir(path):\n",
    "        tweet_count+=1   \n",
    "        p2=os.path.join(path,t)          \n",
    "        with open(p2, 'rb') as f:\n",
    "            text = f.read()\n",
    "            text=cumleyiarindir(text)\n",
    "        sentences.append(text)\n",
    "        labels.append(x)  \n",
    "        f.close()\n",
    "    print(x,'. Klasör tweet sayısı :',tweet_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('mail.csv','wt',encoding='utf8')\n",
    "for i in range(len(sentences)):\n",
    "    newSentence = sentences[i]+'\\n'\n",
    "    fp.write(newSentence)\n",
    "fp.close() \n",
    "\n",
    "fp = open('maillabellat.csv','wt',encoding='utf8')\n",
    "for i in range(len(labels)):\n",
    "    newSentence = labels[i]+'\\n'\n",
    "    fp.write(newSentence)\n",
    "fp.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      oz0  oz1  oz2  oz3       oz4  oz5  oz6  oz7  oz8       oz9  ...  oz2513  \\\n",
      "0     0.0  0.0  0.0  0.0  1.028098  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "...   ...  ...  ...  ...       ...  ...  ...  ...  ...       ...  ...     ...   \n",
      "1330  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "1331  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.686797  ...     0.0   \n",
      "1332  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "1333  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "1334  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.000000  ...     0.0   \n",
      "\n",
      "      oz2514  oz2515  oz2516  oz2517  oz2518  oz2519  oz2520  oz2521  \\\n",
      "0        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "2        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "3        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "4        0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "1330     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1331     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1332     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1333     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "1334     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "      twitLabels  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "...          ...  \n",
      "1330           0  \n",
      "1331           0  \n",
      "1332           0  \n",
      "1333           1  \n",
      "1334           1  \n",
      "\n",
      "[1335 rows x 2523 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimport numpy as np\\ndata.columns=['cumle','etiket']\\nindex = data['cumle'].index[data['cumle'].apply(pd.isnull)]\\nprint(index[0])\\nfind = data.any 1036,1405,7694,18230,\\nprint(find) 545,1004,1006,1008,1035,1403,7691,9800,18226\\nimport numpy as np a.bool(), a.item(), a.any()\\ndf1 = data.replace(np.nan, '', regex=True)\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "#tweetdata.csv\n",
    "#calisirmilan.csv   nan değerler silindi \n",
    "#tweetdata2.csv sep=; parametresini kaldır \n",
    "\n",
    "csvname ='stsGold.txt'\n",
    "data  = pd.read_csv(csvname ,sep=',' , engine='python')\n",
    "print(data)\n",
    "\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "data.columns=['cumle','etiket']\n",
    "index = data['cumle'].index[data['cumle'].apply(pd.isnull)]\n",
    "print(index[0])\n",
    "find = data.any 1036,1405,7694,18230,\n",
    "print(find) 545,1004,1006,1008,1035,1403,7691,9800,18226\n",
    "import numpy as np a.bool(), a.item(), a.any()\n",
    "df1 = data.replace(np.nan, '', regex=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences = [ doc for doc in data.iloc[:,0]]\n",
    "#labels = [ doc for doc in data.iloc[:,1]]\n",
    "sentences= data.iloc[:,:-1]\n",
    "labels=data['twitLabels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word',lowercase = True)\n",
    "deneme = vectorizer.fit_transform(sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(deneme,labels,test_size=0.80,random_state=38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "classifier= MultinomialNB() #classifier nesnesi oluşturdum\n",
    "model = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '2' '2' ... '2' '2' '2']\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(X_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  71  509   19]\n",
      " [   6 1013    3]\n",
      " [  28  704   47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.12      0.20       599\n",
      "           2       0.46      0.99      0.62      1022\n",
      "           3       0.68      0.06      0.11       779\n",
      "\n",
      "    accuracy                           0.47      2400\n",
      "   macro avg       0.60      0.39      0.31      2400\n",
      "weighted avg       0.58      0.47      0.35      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
